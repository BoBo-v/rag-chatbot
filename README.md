# RAG æœ¬åœ°çŸ¥è¯†åº“é¡¹ç›® - å®Œæ•´æ„å»ºæŒ‡å—

> é€‚åˆï¼šPythoné›¶åŸºç¡€ + æœ‰å‰ç«¯ç»éªŒçš„å¼€å‘è€…

---

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„ï¼ˆç”Ÿäº§çº§ï¼‰

```
rag-chatbot/
â”œâ”€â”€ .env                    # ç¯å¢ƒå˜é‡é…ç½®
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/                   # çŸ¥è¯†åº“åŸå§‹æ–‡æ¡£
â”‚   â”œâ”€â”€ product_intro.txt
â”‚   â”œâ”€â”€ faq.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ vectorstore/            # å‘é‡æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨
â”‚   â””â”€â”€ chroma_db/
â”‚
â”œâ”€â”€ chat_history/           # èŠå¤©è®°å½•å­˜å‚¨
â”‚   â””â”€â”€ sessions/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py             # FastAPI å…¥å£
â”‚   â”œâ”€â”€ config.py           # é…ç½®ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py       # APIè·¯ç”±
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag_engine.py   # RAGæ ¸å¿ƒé€»è¾‘
â”‚   â”‚   â”œâ”€â”€ chat_memory.py  # å¯¹è¯è®°å¿†ç®¡ç†
â”‚   â”‚   â””â”€â”€ vector_store.py # å‘é‡åº“ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py      # æ•°æ®æ¨¡å‹
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ text_loader.py  # æ–‡æ¡£åŠ è½½å·¥å…·
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ init_vectorstore.py # åˆå§‹åŒ–å‘é‡åº“è„šæœ¬
    â””â”€â”€ test_api.py         # æµ‹è¯•è„šæœ¬
```

---

## ğŸ”§ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒæ­å»º

### 1.1 å®‰è£… Pythonï¼ˆæ¨è3.10+ï¼‰

```bash
# macOS
brew install python@3.11

# Windows
# å®˜ç½‘ä¸‹è½½ï¼šhttps://www.python.org/downloads/
# å®‰è£…æ—¶å‹¾é€‰ "Add Python to PATH"

# éªŒè¯
python --version   # æˆ– python3 --version
```

### 1.2 åˆ›å»ºé¡¹ç›®å’Œè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir rag-chatbot
cd rag-chatbot

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆéš”ç¦»ä¾èµ–ï¼Œç±»ä¼¼npmçš„node_modulesï¼‰
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# macOS/Linux:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# æ¿€æ´»åå‘½ä»¤è¡Œä¼šæ˜¾ç¤º (venv) å‰ç¼€
```

### 1.3 å®‰è£…ä¾èµ–

åˆ›å»º `requirements.txt`ï¼š
```txt
# Webæ¡†æ¶
fastapi==0.115.0
uvicorn[standard]==0.30.6
python-multipart==0.0.9

# LangChain ç”Ÿæ€
langchain==0.3.0
langchain-community==0.3.0
langchain-ollama==0.2.0
langchain-text-splitters==0.3.0
langchain-chroma==0.1.4

# å‘é‡æ•°æ®åº“
chromadb==0.5.5

# ä¸­æ–‡åˆ†è¯
jieba==0.42.1

# å·¥å…·
python-dotenv==1.0.1
pydantic==2.9.0
pydantic-settings==2.5.0

# æ•°æ®å­˜å‚¨
aiosqlite==0.20.0  # å¼‚æ­¥SQLiteï¼Œå­˜èŠå¤©è®°å½•
```

å®‰è£…ï¼š
```bash
pip install -r requirements.txt
```

### 1.4 å®‰è£… Ollamaï¼ˆæœ¬åœ°å¤§æ¨¡å‹ï¼‰

```bash
# macOS
brew install ollama

# æˆ–å»å®˜ç½‘ä¸‹è½½ï¼šhttps://ollama.ai

# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# å¦å¼€ç»ˆç«¯ï¼Œä¸‹è½½æ¨¡å‹
ollama pull qwen2.5:7b      # ä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼Œçº¦4.5GB
ollama pull bge-m3          # ä¸­æ–‡å‘é‡æ¨¡å‹ï¼Œçº¦2GB

# éªŒè¯
ollama list
```

---

## ğŸ“ ç¬¬äºŒæ­¥ï¼šåˆ›å»ºé¡¹ç›®æ–‡ä»¶

### 2.1 é…ç½®æ–‡ä»¶ `app/config.py`

```python
"""
é…ç½®ç®¡ç†
ç±»æ¯”JSï¼šç±»ä¼¼äº config.js æˆ– .env é…ç½®
"""
from pydantic_settings import BaseSettings
from pathlib import Path


class Settings(BaseSettings):
    # æœºå™¨äººé…ç½®
    BOT_NAME: str = "å°æ™º"
    BOT_COMPANY: str = "XXXç§‘æŠ€"
    
    # æ¨¡å‹é…ç½®
    LLM_MODEL: str = "qwen2.5:7b"
    EMBEDDING_MODEL: str = "bge-m3"
    OLLAMA_BASE_URL: str = "http://localhost:11434"
    
    # è·¯å¾„é…ç½®
    BASE_DIR: Path = Path(__file__).parent.parent
    DATA_DIR: Path = BASE_DIR / "data"
    VECTOR_DB_DIR: Path = BASE_DIR / "vectorstore" / "chroma_db"
    CHAT_HISTORY_DIR: Path = BASE_DIR / "chat_history"
    
    # RAGé…ç½®
    CHUNK_SIZE: int = 500
    CHUNK_OVERLAP: int = 100
    RETRIEVER_K: int = 5
    
    # å¯¹è¯é…ç½®
    MAX_HISTORY_TURNS: int = 10  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
    
    class Config:
        env_file = ".env"


settings = Settings()
```

### 2.2 æ•°æ®æ¨¡å‹ `app/models/schemas.py`

```python
"""
Pydantic æ•°æ®æ¨¡å‹
ç±»æ¯”JSï¼šç±»ä¼¼äº TypeScript çš„ interface/type
"""
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


# ========== è¯·æ±‚æ¨¡å‹ ==========
class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    question: str = Field(..., min_length=1, description="ç”¨æˆ·é—®é¢˜")
    session_id: Optional[str] = Field(None, description="ä¼šè¯IDï¼Œç”¨äºè¿ç»­å¯¹è¯")


class CreateSessionRequest(BaseModel):
    """åˆ›å»ºä¼šè¯è¯·æ±‚"""
    user_id: Optional[str] = None
    metadata: Optional[dict] = None


# ========== å“åº”æ¨¡å‹ ==========
class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    answer: str
    session_id: str
    sources: list[str] = []
    timestamp: datetime = Field(default_factory=datetime.now)


class SessionInfo(BaseModel):
    """ä¼šè¯ä¿¡æ¯"""
    session_id: str
    created_at: datetime
    message_count: int
    last_message_at: Optional[datetime] = None


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    llm_model: str
    embedding_model: str
    chunks_count: int
    sessions_count: int
```

### 2.3 å¯¹è¯è®°å¿†ç®¡ç† `app/core/chat_memory.py`

```python
"""
å¯¹è¯è®°å¿†ç®¡ç† - å®ç°è¿ç»­å¯¹è¯å’ŒèŠå¤©å­˜å‚¨
è¿™æ˜¯ä½ éœ€è¦çš„æ ¸å¿ƒåŠŸèƒ½ï¼
"""
import json
import uuid
import aiosqlite
from datetime import datetime
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, asdict

from langchain_core.messages import HumanMessage, AIMessage, BaseMessage


@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str  # "human" or "ai"
    content: str
    timestamp: str
    

class ChatMemoryManager:
    """
    èŠå¤©è®°å¿†ç®¡ç†å™¨
    - ç®¡ç†å¤šä¸ªä¼šè¯
    - æŒä¹…åŒ–å­˜å‚¨èŠå¤©è®°å½•
    - æ”¯æŒä¸Šä¸‹æ–‡çª—å£æ§åˆ¶
    """
    
    def __init__(self, db_path: str, max_history: int = 10):
        self.db_path = db_path
        self.max_history = max_history
        self._ensure_dir()
    
    def _ensure_dir(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
    
    async def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        async with aiosqlite.connect(self.db_path) as db:
            # ä¼šè¯è¡¨
            await db.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    session_id TEXT PRIMARY KEY,
                    user_id TEXT,
                    created_at TEXT,
                    updated_at TEXT,
                    metadata TEXT
                )
            """)
            # æ¶ˆæ¯è¡¨
            await db.execute("""
                CREATE TABLE IF NOT EXISTS messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT,
                    role TEXT,
                    content TEXT,
                    timestamp TEXT,
                    FOREIGN KEY (session_id) REFERENCES sessions(session_id)
                )
            """)
            # ç´¢å¼•
            await db.execute("""
                CREATE INDEX IF NOT EXISTS idx_messages_session 
                ON messages(session_id)
            """)
            await db.commit()
    
    async def create_session(
        self, 
        user_id: Optional[str] = None,
        metadata: Optional[dict] = None
    ) -> str:
        """åˆ›å»ºæ–°ä¼šè¯"""
        session_id = str(uuid.uuid4())
        now = datetime.now().isoformat()
        
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(
                """INSERT INTO sessions 
                   (session_id, user_id, created_at, updated_at, metadata) 
                   VALUES (?, ?, ?, ?, ?)""",
                (session_id, user_id, now, now, json.dumps(metadata or {}))
            )
            await db.commit()
        
        return session_id
    
    async def add_message(
        self, 
        session_id: str, 
        role: str, 
        content: str
    ):
        """æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯"""
        now = datetime.now().isoformat()
        
        async with aiosqlite.connect(self.db_path) as db:
            # æ·»åŠ æ¶ˆæ¯
            await db.execute(
                """INSERT INTO messages (session_id, role, content, timestamp) 
                   VALUES (?, ?, ?, ?)""",
                (session_id, role, content, now)
            )
            # æ›´æ–°ä¼šè¯æ—¶é—´
            await db.execute(
                "UPDATE sessions SET updated_at = ? WHERE session_id = ?",
                (now, session_id)
            )
            await db.commit()
    
    async def get_history(
        self, 
        session_id: str,
        limit: Optional[int] = None
    ) -> list[ChatMessage]:
        """è·å–ä¼šè¯å†å²"""
        limit = limit or self.max_history
        
        async with aiosqlite.connect(self.db_path) as db:
            db.row_factory = aiosqlite.Row
            cursor = await db.execute(
                """SELECT role, content, timestamp FROM messages 
                   WHERE session_id = ? 
                   ORDER BY id DESC LIMIT ?""",
                (session_id, limit * 2)  # æ¯è½®2æ¡æ¶ˆæ¯
            )
            rows = await cursor.fetchall()
        
        # åè½¬é¡ºåºï¼ˆä»æ—§åˆ°æ–°ï¼‰
        messages = [
            ChatMessage(role=row["role"], content=row["content"], timestamp=row["timestamp"])
            for row in reversed(rows)
        ]
        return messages
    
    async def get_langchain_history(self, session_id: str) -> list[BaseMessage]:
        """è·å–LangChainæ ¼å¼çš„å†å²æ¶ˆæ¯ï¼ˆç”¨äºä¼ ç»™LLMï¼‰"""
        messages = await self.get_history(session_id)
        
        lc_messages = []
        for msg in messages:
            if msg.role == "human":
                lc_messages.append(HumanMessage(content=msg.content))
            else:
                lc_messages.append(AIMessage(content=msg.content))
        
        return lc_messages
    
    async def session_exists(self, session_id: str) -> bool:
        """æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨"""
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute(
                "SELECT 1 FROM sessions WHERE session_id = ?",
                (session_id,)
            )
            return await cursor.fetchone() is not None
    
    async def get_session_count(self) -> int:
        """è·å–ä¼šè¯æ€»æ•°"""
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("SELECT COUNT(*) FROM sessions")
            result = await cursor.fetchone()
            return result[0] if result else 0
    
    async def get_all_sessions(self) -> list[dict]:
        """è·å–æ‰€æœ‰ä¼šè¯åˆ—è¡¨"""
        async with aiosqlite.connect(self.db_path) as db:
            db.row_factory = aiosqlite.Row
            cursor = await db.execute(
                """SELECT s.session_id, s.created_at, s.updated_at,
                          COUNT(m.id) as message_count
                   FROM sessions s
                   LEFT JOIN messages m ON s.session_id = m.session_id
                   GROUP BY s.session_id
                   ORDER BY s.updated_at DESC"""
            )
            rows = await cursor.fetchall()
        
        return [dict(row) for row in rows]


# ========== ä¸Šä¸‹æ–‡æ ¼å¼åŒ–å·¥å…· ==========
def format_history_for_prompt(messages: list[ChatMessage], max_chars: int = 2000) -> str:
    """
    å°†å†å²æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºpromptå­—ç¬¦ä¸²
    æ§åˆ¶é•¿åº¦é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£
    """
    if not messages:
        return ""
    
    lines = []
    total_chars = 0
    
    # ä»æœ€è¿‘çš„æ¶ˆæ¯å¼€å§‹ï¼Œå€’åºæ·»åŠ 
    for msg in reversed(messages):
        role_name = "ç”¨æˆ·" if msg.role == "human" else "åŠ©æ‰‹"
        line = f"{role_name}: {msg.content}"
        
        if total_chars + len(line) > max_chars:
            break
        
        lines.insert(0, line)
        total_chars += len(line)
    
    return "\n".join(lines)
```

### 2.4 å‘é‡åº“ç®¡ç† `app/core/vector_store.py`

```python
"""
å‘é‡æ•°æ®åº“ç®¡ç†
è´Ÿè´£ï¼šæ–‡æ¡£åŠ è½½ã€åˆ†å—ã€å‘é‡åŒ–ã€æ£€ç´¢
"""
from pathlib import Path
from typing import Optional

from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

from app.config import settings


class VectorStoreManager:
    """å‘é‡åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.embeddings = OllamaEmbeddings(
            model=settings.EMBEDDING_MODEL,
            base_url=settings.OLLAMA_BASE_URL
        )
        self.vectordb: Optional[Chroma] = None
        self.chunks_count = 0
    
    def load_documents(self, data_dir: Path) -> list[Document]:
        """ä»ç›®å½•åŠ è½½æ–‡æ¡£"""
        documents = []
        
        for file_path in data_dir.glob("*"):
            if file_path.suffix in [".txt", ".md"]:
                try:
                    content = file_path.read_text(encoding="utf-8")
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": file_path.name,
                            "file_type": file_path.suffix
                        }
                    )
                    documents.append(doc)
                    print(f"  âœ“ åŠ è½½: {file_path.name}")
                except Exception as e:
                    print(f"  âœ— åŠ è½½å¤±è´¥ {file_path.name}: {e}")
        
        return documents
    
    def split_documents(self, documents: list[Document]) -> list[Document]:
        """æ–‡æ¡£åˆ†å—"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
        )
        chunks = splitter.split_documents(documents)
        return chunks
    
    def create_vectorstore(self, chunks: list[Document], persist: bool = True):
        """åˆ›å»ºå‘é‡æ•°æ®åº“"""
        persist_dir = str(settings.VECTOR_DB_DIR) if persist else None
        
        self.vectordb = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=persist_dir,
            collection_name="knowledge_base"
        )
        self.chunks_count = len(chunks)
        print(f"âœ… å‘é‡åº“åˆ›å»ºå®Œæˆï¼Œå…± {self.chunks_count} ä¸ªæ–‡æ¡£å—")
    
    def load_vectorstore(self):
        """åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“"""
        persist_dir = str(settings.VECTOR_DB_DIR)
        
        if not Path(persist_dir).exists():
            raise FileNotFoundError(f"å‘é‡åº“ä¸å­˜åœ¨: {persist_dir}")
        
        self.vectordb = Chroma(
            persist_directory=persist_dir,
            embedding_function=self.embeddings,
            collection_name="knowledge_base"
        )
        # è·å–æ–‡æ¡£æ•°é‡
        self.chunks_count = self.vectordb._collection.count()
        print(f"âœ… å‘é‡åº“åŠ è½½å®Œæˆï¼Œå…± {self.chunks_count} ä¸ªæ–‡æ¡£å—")
    
    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨"""
        if not self.vectordb:
            raise RuntimeError("å‘é‡åº“æœªåˆå§‹åŒ–")
        
        return self.vectordb.as_retriever(
            search_type="mmr",  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼Œå¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
            search_kwargs={
                "k": settings.RETRIEVER_K,
                "fetch_k": 20,
                "lambda_mult": 0.7
            }
        )
    
    def search(self, query: str, k: int = 5) -> list[Document]:
        """ç›´æ¥æœç´¢"""
        if not self.vectordb:
            raise RuntimeError("å‘é‡åº“æœªåˆå§‹åŒ–")
        return self.vectordb.similarity_search(query, k=k)


# å…¨å±€å•ä¾‹
vector_manager = VectorStoreManager()
```

### 2.5 RAGå¼•æ“ `app/core/rag_engine.py`

```python
"""
RAG æ ¸å¿ƒå¼•æ“
æ•´åˆï¼šå‘é‡æ£€ç´¢ + å¯¹è¯å†å² + LLMç”Ÿæˆ
"""
from typing import AsyncGenerator, Optional

from langchain_ollama import OllamaLLM

from app.config import settings
from app.core.vector_store import vector_manager
from app.core.chat_memory import ChatMemoryManager, format_history_for_prompt


class RAGEngine:
    """RAG é—®ç­”å¼•æ“"""
    
    def __init__(self, memory_manager: ChatMemoryManager):
        self.memory = memory_manager
        self.llm = OllamaLLM(
            model=settings.LLM_MODEL,
            base_url=settings.OLLAMA_BASE_URL,
            temperature=0.7,
            num_predict=1024
        )
        self.retriever = vector_manager.get_retriever()
    
    def _build_prompt(
        self, 
        question: str, 
        context: str, 
        history: str = ""
    ) -> str:
        """æ„å»ºå®Œæ•´çš„ Prompt"""
        
        history_section = ""
        if history:
            history_section = f"""
å†å²å¯¹è¯ï¼ˆç”¨äºç†è§£ä¸Šä¸‹æ–‡ï¼‰:
{history}
"""
        
        prompt = f"""ä½ æ˜¯"{settings.BOT_NAME}"ï¼Œä¸€ä¸ªä¸“ä¸šå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”±{settings.BOT_COMPANY}å¼€å‘ã€‚

èº«ä»½è§„åˆ™ï¼š
- ä½ çš„åå­—æ˜¯ï¼š{settings.BOT_NAME}
- ä½ çš„å¼€å‘è€…æ˜¯ï¼š{settings.BOT_COMPANY}
- å½“è¢«é—®åˆ°èº«ä»½ç›¸å…³é—®é¢˜æ—¶ï¼Œä½¿ç”¨ä¸Šè¿°ä¿¡æ¯å›ç­”

å›ç­”è§„åˆ™ï¼š
1. æ ¹æ®ã€çŸ¥è¯†åº“å†…å®¹ã€‘å›ç­”é—®é¢˜
2. å¦‚æœçŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®è¯´"æˆ‘ä¸å¤ªæ¸…æ¥šè¿™ä¸ªé—®é¢˜"
3. ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”
4. å›ç­”è¦ç®€æ´æ˜äº†
{history_section}
ã€çŸ¥è¯†åº“å†…å®¹ã€‘:
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘: {question}

ã€å›ç­”ã€‘:"""
        
        return prompt
    
    async def ask(
        self, 
        question: str, 
        session_id: str
    ) -> tuple[str, list[str]]:
        """
        å¤„ç†é—®é¢˜ï¼ˆéæµå¼ï¼‰
        è¿”å›ï¼š(ç­”æ¡ˆ, æ¥æºåˆ—è¡¨)
        """
        # 1. è·å–å†å²å¯¹è¯
        history_messages = await self.memory.get_history(session_id)
        history_text = format_history_for_prompt(history_messages)
        
        # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.retriever.invoke(question)
        context = "\n\n---\n\n".join(doc.page_content for doc in docs)
        
        # 3. æ„å»º prompt
        prompt = self._build_prompt(question, context, history_text)
        
        # 4. è°ƒç”¨ LLM
        answer = self.llm.invoke(prompt)
        
        # 5. ä¿å­˜å¯¹è¯
        await self.memory.add_message(session_id, "human", question)
        await self.memory.add_message(session_id, "ai", answer)
        
        # 6. æå–æ¥æº
        sources = [
            f"[{doc.metadata.get('source', 'æœªçŸ¥')}] {doc.page_content[:80]}..."
            for doc in docs[:3]
        ]
        
        return answer, sources
    
    async def ask_stream(
        self, 
        question: str, 
        session_id: str
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼é—®ç­”
        """
        # 1. è·å–å†å²å¯¹è¯
        history_messages = await self.memory.get_history(session_id)
        history_text = format_history_for_prompt(history_messages)
        
        # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.retriever.invoke(question)
        context = "\n\n---\n\n".join(doc.page_content for doc in docs)
        
        # 3. æ„å»º prompt
        prompt = self._build_prompt(question, context, history_text)
        
        # 4. æµå¼ç”Ÿæˆ
        full_answer = ""
        for chunk in self.llm.stream(prompt):
            full_answer += chunk
            yield chunk
        
        # 5. ä¿å­˜å®Œæ•´å¯¹è¯
        await self.memory.add_message(session_id, "human", question)
        await self.memory.add_message(session_id, "ai", full_answer)
```

### 2.6 API è·¯ç”± `app/api/routes.py`

```python
"""
API è·¯ç”±å®šä¹‰
"""
import asyncio
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse

from app.models.schemas import (
    ChatRequest, ChatResponse, 
    CreateSessionRequest, SessionInfo, HealthResponse
)
from app.core.rag_engine import RAGEngine
from app.core.chat_memory import ChatMemoryManager
from app.core.vector_store import vector_manager
from app.config import settings


router = APIRouter()

# åˆå§‹åŒ–ç»„ä»¶ï¼ˆä¼šåœ¨main.pyä¸­è°ƒç”¨ï¼‰
memory_manager: ChatMemoryManager = None
rag_engine: RAGEngine = None


async def init_components():
    """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
    global memory_manager, rag_engine
    
    # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
    db_path = str(settings.CHAT_HISTORY_DIR / "chat.db")
    memory_manager = ChatMemoryManager(db_path, max_history=settings.MAX_HISTORY_TURNS)
    await memory_manager.init_db()
    
    # åŠ è½½å‘é‡åº“
    try:
        vector_manager.load_vectorstore()
    except FileNotFoundError:
        print("âš ï¸ å‘é‡åº“ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
        docs = vector_manager.load_documents(settings.DATA_DIR)
        if docs:
            chunks = vector_manager.split_documents(docs)
            vector_manager.create_vectorstore(chunks)
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†åº“æ–‡æ¡£")
    
    # åˆå§‹åŒ– RAG å¼•æ“
    rag_engine = RAGEngine(memory_manager)
    
    print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")


# ========== ä¼šè¯ç®¡ç† ==========

@router.post("/sessions", response_model=SessionInfo)
async def create_session(request: CreateSessionRequest = None):
    """åˆ›å»ºæ–°ä¼šè¯"""
    request = request or CreateSessionRequest()
    session_id = await memory_manager.create_session(
        user_id=request.user_id,
        metadata=request.metadata
    )
    return SessionInfo(
        session_id=session_id,
        created_at=__import__('datetime').datetime.now(),
        message_count=0
    )


@router.get("/sessions")
async def list_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    sessions = await memory_manager.get_all_sessions()
    return {"sessions": sessions}


@router.get("/sessions/{session_id}/history")
async def get_session_history(session_id: str):
    """è·å–ä¼šè¯å†å²"""
    if not await memory_manager.session_exists(session_id):
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    messages = await memory_manager.get_history(session_id, limit=50)
    return {
        "session_id": session_id,
        "messages": [
            {"role": m.role, "content": m.content, "timestamp": m.timestamp}
            for m in messages
        ]
    }


# ========== èŠå¤©æ¥å£ ==========

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    èŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰
    
    - å¦‚æœä¸ä¼  session_idï¼Œä¼šè‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯
    - ä¼ å…¥ session_id åˆ™ç»§ç»­ä¹‹å‰çš„å¯¹è¯
    """
    # å¤„ç†ä¼šè¯
    session_id = request.session_id
    if not session_id:
        session_id = await memory_manager.create_session()
    elif not await memory_manager.session_exists(session_id):
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    try:
        answer, sources = await rag_engine.ask(request.question, session_id)
        return ChatResponse(
            answer=answer,
            session_id=session_id,
            sources=sources
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼èŠå¤©æ¥å£
    è¿”å› SSE (Server-Sent Events)
    """
    # å¤„ç†ä¼šè¯
    session_id = request.session_id
    if not session_id:
        session_id = await memory_manager.create_session()
    elif not await memory_manager.session_exists(session_id):
        raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
    
    async def generate():
        # å…ˆå‘é€ session_id
        yield f"data: {{'type': 'session', 'session_id': '{session_id}'}}\n\n"
        
        try:
            async for chunk in rag_engine.ask_stream(request.question, session_id):
                # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
                escaped = chunk.replace('\n', '\\n').replace('"', '\\"')
                yield f"data: {{'type': 'content', 'text': \"{escaped}\"}}\n\n"
                await asyncio.sleep(0.01)
            
            yield "data: {\"type\": \"done\"}\n\n"
        except Exception as e:
            yield f"data: {{'type': 'error', 'message': '{str(e)}'}}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


# ========== ç³»ç»Ÿæ¥å£ ==========

@router.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    session_count = await memory_manager.get_session_count()
    return HealthResponse(
        status="healthy",
        llm_model=settings.LLM_MODEL,
        embedding_model=settings.EMBEDDING_MODEL,
        chunks_count=vector_manager.chunks_count,
        sessions_count=session_count
    )
```

### 2.7 ä¸»å…¥å£ `app/main.py`

```python
"""
FastAPI åº”ç”¨å…¥å£
"""
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.api.routes import router, init_components
from app.config import settings


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ RAG ç³»ç»Ÿ...")
    await init_components()
    print(f"âœ… {settings.BOT_NAME} å‡†å¤‡å°±ç»ª!")
    
    yield
    
    # å…³é—­æ—¶
    print("ğŸ‘‹ ç³»ç»Ÿå…³é—­")


# åˆ›å»ºåº”ç”¨
app = FastAPI(
    title=f"{settings.BOT_NAME} API",
    description="æœ¬åœ°çŸ¥è¯†åº“æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    version="1.0.0",
    lifespan=lifespan
)

# è·¨åŸŸé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒæ”¹æˆå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(router, prefix="/api")


# æ ¹è·¯å¾„
@app.get("/")
def root():
    return {
        "name": settings.BOT_NAME,
        "company": settings.BOT_COMPANY,
        "status": "running",
        "docs": "/docs"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True  # å¼€å‘æ¨¡å¼ï¼Œä»£ç ä¿®æ”¹è‡ªåŠ¨é‡å¯
    )
```

---

## ğŸš€ ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œé¡¹ç›®

### 3.1 å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£

åœ¨ `data/` ç›®å½•ä¸‹æ”¾å…¥ä½ çš„çŸ¥è¯†æ–‡æ¡£ï¼š

```bash
mkdir -p data
echo "è¿™æ˜¯äº§å“ä»‹ç»æ–‡æ¡£..." > data/product.txt
echo "Q: ä½ ä»¬çš„æœåŠ¡æ—¶é—´ï¼Ÿ\nA: å‘¨ä¸€åˆ°å‘¨äº” 9:00-18:00" > data/faq.md
```

### 3.2 åˆ›å»ºå¿…è¦ç›®å½•

```bash
mkdir -p vectorstore chat_history app/api app/core app/models app/utils scripts

# åˆ›å»º __init__.py æ–‡ä»¶ï¼ˆPythonåŒ…æ ‡è¯†ï¼‰
touch app/__init__.py
touch app/api/__init__.py
touch app/core/__init__.py
touch app/models/__init__.py
touch app/utils/__init__.py
```

### 3.3 å¯åŠ¨æœåŠ¡

```bash
# ç¡®ä¿ Ollama åœ¨è¿è¡Œ
ollama serve

# å¯åŠ¨ API æœåŠ¡
python -m app.main

# æˆ–ä½¿ç”¨ uvicorn
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 3.4 æµ‹è¯•æ¥å£

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/health

# åˆ›å»ºä¼šè¯
curl -X POST http://localhost:8000/api/sessions

# å‘é€æ¶ˆæ¯ï¼ˆå¸¦ä¸Šsession_idå®ç°è¿ç»­å¯¹è¯ï¼‰
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ", "session_id": "ä½ çš„session_id"}'

# æŸ¥çœ‹å†å²
curl http://localhost:8000/api/sessions/{session_id}/history
```

---

## ğŸ“Š API æ¥å£æ–‡æ¡£

å¯åŠ¨åè®¿é—®ï¼š`http://localhost:8000/docs`ï¼ˆSwagger UIè‡ªåŠ¨ç”Ÿæˆï¼‰

| æ¥å£ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/api/sessions` | POST | åˆ›å»ºæ–°ä¼šè¯ |
| `/api/sessions` | GET | è·å–æ‰€æœ‰ä¼šè¯ |
| `/api/sessions/{id}/history` | GET | è·å–ä¼šè¯å†å² |
| `/api/chat` | POST | å‘é€æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰ |
| `/api/chat/stream` | POST | å‘é€æ¶ˆæ¯ï¼ˆæµå¼ï¼‰ |
| `/api/health` | GET | å¥åº·æ£€æŸ¥ |

---

## ğŸ¯ å‰ç«¯å¯¹æ¥ç¤ºä¾‹ï¼ˆä½ ç†Ÿæ‚‰çš„JSï¼‰

```javascript
// åˆ›å»ºä¼šè¯
async function createSession() {
  const res = await fetch('http://localhost:8000/api/sessions', {
    method: 'POST'
  });
  const data = await res.json();
  return data.session_id;
}

// å‘é€æ¶ˆæ¯
async function sendMessage(sessionId, question) {
  const res = await fetch('http://localhost:8000/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ question, session_id: sessionId })
  });
  return await res.json();
}

// æµå¼æ¥æ”¶
async function streamChat(sessionId, question, onChunk) {
  const res = await fetch('http://localhost:8000/api/chat/stream', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ question, session_id: sessionId })
  });
  
  const reader = res.body.getReader();
  const decoder = new TextDecoder();
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const text = decoder.decode(value);
    const lines = text.split('\n');
    
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        try {
          const data = JSON.parse(line.slice(6));
          if (data.type === 'content') {
            onChunk(data.text);
          }
        } catch (e) {}
      }
    }
  }
}
```

---

## ğŸ”§ åæœŸç»´æŠ¤ä¸æ‰©å±•

### æ‰©å±•1: æ·»åŠ ç”¨æˆ·è®¤è¯
```python
# ä½¿ç”¨ fastapi-users æˆ– JWT
pip install python-jose[cryptography] passlib[bcrypt]
```

### æ‰©å±•2: çŸ¥è¯†åº“ç®¡ç†æ¥å£
```python
# æ·»åŠ ä¸Šä¼ æ–‡æ¡£ã€åˆ é™¤æ–‡æ¡£çš„API
@router.post("/knowledge/upload")
async def upload_document(file: UploadFile):
    ...
```

### æ‰©å±•3: å¤šç§Ÿæˆ·æ”¯æŒ
```python
# åœ¨ session è¡¨ä¸­æ·»åŠ  tenant_id
# å‘é‡åº“ä½¿ç”¨ä¸åŒçš„ collection
```

### æ‰©å±•4: ç›‘æ§ä¸æ—¥å¿—
```python
pip install loguru prometheus-fastapi-instrumentator
```

---
